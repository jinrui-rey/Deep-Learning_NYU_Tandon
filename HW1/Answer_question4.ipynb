{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d519be86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "plt.imshow(x_train[2],cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577b172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # Numerically stable sigmoid function based on\n",
    "    # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "\n",
    "    x = np.clip(x, -500, 500)  # We get an overflow warning without this\n",
    "\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )\n",
    "\n",
    "\n",
    "def dsigmoid(x):  # Derivative of sigmoid\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    # Numerically stable softmax based on (same source as sigmoid)\n",
    "    # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "def cross_entropy_loss(y, yHat):\n",
    "    return -np.sum(y * np.log(yHat))\n",
    "\n",
    "def integer_to_one_hot(x, max):\n",
    "    # x: integer to convert to one hot encoding\n",
    "    # max: the size of the one hot encoded array\n",
    "    result = np.zeros(10)\n",
    "    result[x] = 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c518a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Initialize weights of each layer with a normal distribution of mean 0 and\n",
    "# standard deviation 1/sqrt(n), where n is the number of inputs.\n",
    "# This means the weighted input will be a random variable itself with mean\n",
    "# 0 and standard deviation close to 1 (if biases are initialized as 0, standard\n",
    "# deviation will be exactly 1)\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(80085)\n",
    "\n",
    "# Q1. Fill initialization code here.\n",
    "# ...\n",
    "\n",
    "layers = [784, 32, 32, 10]\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    n_in, n_out = layers[i], layers[i + 1]\n",
    "    w = rng.normal(0, (1 / math.sqrt(n_in)), (n_in, n_out))\n",
    "    weights.append(w)\n",
    "\n",
    "    b = np.zeros((1,n_out))\n",
    "    biases.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677a9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_sample(sample, label):\n",
    "    \"\"\"\n",
    "    Forward pass through the neural network.\n",
    "    Inputs:\n",
    "      sample: 1D numpy array. The input sample (an MNIST digit).\n",
    "      label: An integer from 0 to 9.\n",
    "\n",
    "    Returns: the cross entropy loss, most likely class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Q2. Fill code here.\n",
    "    # ...\n",
    "    \n",
    "    activation = sample.flatten()\n",
    "    \n",
    "    for index, weight in enumerate(weights):\n",
    "        \n",
    "        z = np.matmul(activation,weight)+biases[index] # z = wx+b\n",
    "        if index < len(weights)-1:\n",
    "            activation = sigmoid(z)  # z' = sigmoid(z)\n",
    "        else:\n",
    "            activation = softmax(z)  # z' = softmax(z) \n",
    "\n",
    "    y = integer_to_one_hot(label,10)  # True label\n",
    "    \n",
    "    loss = cross_entropy_loss(y,activation) \n",
    "    \n",
    "    prediction = np.argmax(activation)\n",
    "    one_hot_prediction = integer_to_one_hot(prediction,10)\n",
    "\n",
    "    return loss, one_hot_prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227c50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_dataset(x, y):\n",
    "    losses = np.empty(x.shape[0])\n",
    "    one_hot_guesses = np.empty((x.shape[0], 10))\n",
    "    # ...\n",
    "    # Q2. Fill code here to calculate losses, one_hot_guesses\n",
    "    # ...\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        sample = x[i]\n",
    "        label = y[i]\n",
    "        L, P = feed_forward_sample(sample, label)\n",
    "        losses[i] = L\n",
    "        one_hot_guesses[i] = P\n",
    "\n",
    "    y_one_hot = np.zeros((y.size, 10))\n",
    "    y_one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "    correct_guesses = np.sum(y_one_hot * one_hot_guesses)\n",
    "    correct_guess_percent = format((correct_guesses / y.shape[0]) * 100, \".2f\")\n",
    "\n",
    "    print(\"\\nAverage loss:\", np.round(np.average(losses), decimals=2))\n",
    "    print(\"Accuracy (# of correct guesses):\", correct_guesses, \"/\", y.shape[0], \"(\", correct_guess_percent, \"%)\")\n",
    "\n",
    "\n",
    "def feed_forward_training_data():\n",
    "    print(\"Feeding forward all training data...\")\n",
    "    feed_forward_dataset(x_train, y_train)\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def feed_forward_test_data():\n",
    "    print(\"Feeding forward all test data...\")\n",
    "    feed_forward_dataset(x_test, y_test)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ceb9c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 2.37\n",
      "Accuracy (# of correct guesses): 880.0 / 10000 ( 8.80 %)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feed_forward_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e599d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_sample(sample, y, learning_rate=0.003): \n",
    "    a = np.reshape(sample,(1,28*28))\n",
    "      \n",
    "    # We will store each layer's activations to calculate gradient\n",
    "    \n",
    "    # Forward pass\n",
    "    activations = []\n",
    "    num_layers = 3\n",
    "    weight_gradients = []\n",
    "    bias_gradients = []\n",
    "    numclasses = 10\n",
    "    \n",
    "    u1 = np.dot(a,weights[0]) + biases[0]\n",
    "    z1 = sigmoid(u1)  # hidden layer1\n",
    "    activations.append(z1)\n",
    "    \n",
    "    u2 = np.dot(z1,weights[1]) + biases[1]\n",
    "    z2 = sigmoid(u2)  # hidden layer2\n",
    "    activations.append(z2)\n",
    "    \n",
    "    u3 = np.dot(z2,weights[2]) + biases[2]\n",
    "    yhat = softmax(u3)  # hidden layer3\n",
    "    activations.append(yhat)\n",
    "    \n",
    "    #activation[i][j] indicate the activated result of j_th neuron in the i_th layer\n",
    "        \n",
    "    # Q3. This should be the same as what you did in feed_forward_sample above.\n",
    "    # ...    \n",
    "    \n",
    "    # Loss Calculation\n",
    "    yvector = integer_to_one_hot(y,numclasses)\n",
    "    loss = cross_entropy_loss(yvector,yhat)\n",
    "    \n",
    "    #Prediction\n",
    "    pred = np.argmax(yhat)\n",
    "    one_hot_guess = integer_to_one_hot(pred,10)\n",
    "        \n",
    "    # Backward pass\n",
    "\n",
    "    # Q3. Implement backpropagation by backward-stepping gradients through each layer.\n",
    "    # You may need to be careful to make sure your Jacobian matrices are the right shape.\n",
    "    # At the end, you should get two vectors: weight_gradients and bias_gradients.\n",
    "    # ...\n",
    "    \n",
    "    # Update weights & biases based on your calculated gradient\n",
    "    \n",
    "    du3 = activations[2]-yvector  # hidden layer3\n",
    "    dW3 = activations[1].T.dot(du3)\n",
    "    db3 = du3\n",
    "    \n",
    "    dz2 = du3.dot(weights[2].T)  # hidden layer2\n",
    "    dW2 = activations[0].T.dot(np.multiply(dz2,dsigmoid(u2)))\n",
    "    db2 = np.multiply(dz2,dsigmoid(u2))\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(weights[1].T),dsigmoid(u2))  # hidden layer1\n",
    "    dW1 = a.T.dot(np.multiply(dz1,dsigmoid(u1)))\n",
    "    db1 = np.multiply(dz1,dsigmoid(u1))\n",
    "                                       \n",
    "    weight_gradients.append(dW1)\n",
    "    weight_gradients.append(dW2)\n",
    "    weight_gradients.append(dW3)\n",
    "    bias_gradients.append(db1)\n",
    "    bias_gradients.append(db2)\n",
    "    bias_gradients.append(db3)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        weights[i] -= weight_gradients[i] * learning_rate \n",
    "        biases[i] -= bias_gradients[i].flatten() * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4658a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running\n",
      "Training Percentage: 0.002\n",
      "Training Percentage: 16.667\n",
      "Training Percentage: 33.333\n",
      "Training Percentage: 50.000\n",
      "Training Percentage: 66.667\n",
      "Training Percentage: 83.333\n",
      "Training Percentage: 100.000\n",
      "\n",
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 0.49\n",
      "Accuracy (# of correct guesses): 8686.0 / 10000 ( 86.86 %)\n",
      "\n",
      "Epoch 2 is running\n",
      "Training Percentage: 0.002\n",
      "Training Percentage: 16.667\n",
      "Training Percentage: 33.333\n",
      "Training Percentage: 50.000\n",
      "Training Percentage: 66.667\n",
      "Training Percentage: 83.333\n",
      "Training Percentage: 100.000\n",
      "\n",
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 0.45\n",
      "Accuracy (# of correct guesses): 8684.0 / 10000 ( 86.84 %)\n",
      "\n",
      "Epoch 3 is running\n",
      "Training Percentage: 0.002\n",
      "Training Percentage: 16.667\n",
      "Training Percentage: 33.333\n",
      "Training Percentage: 50.000\n",
      "Training Percentage: 66.667\n",
      "Training Percentage: 83.333\n",
      "Training Percentage: 100.000\n",
      "\n",
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 0.46\n",
      "Accuracy (# of correct guesses): 8627.0 / 10000 ( 86.27 %)\n",
      "\n",
      "3 epoches have been completed. \n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(x, y, learning_rate=0.0005):\n",
    "    \n",
    "    # Q4. Write the training loop over the epoch here.\n",
    "    # ...\n",
    "    for i in range(x.shape[0]):\n",
    "        if i==0 or ((i+1)%10000 == 0):\n",
    "            percentage = (i+1)/x_train.shape[0]*100\n",
    "            print('Training Percentage: {:.3f}'.format(percentage))\n",
    "            \n",
    "        train_one_sample(x[i],y[i],learning_rate)\n",
    "    print(\"\")\n",
    "            \n",
    "def test_and_train():\n",
    "    train_one_epoch(x_train,y_train)\n",
    "    feed_forward_test_data()\n",
    "\n",
    "\n",
    "for i in range(3): \n",
    "    print('Epoch {} is running'.format(i+1))\n",
    "    test_and_train()\n",
    "    \n",
    "print('{} epoches have been completed. '.format(i+1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
