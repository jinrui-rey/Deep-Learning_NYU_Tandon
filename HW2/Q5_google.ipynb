{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3137f59",
   "metadata": {
    "id": "e3137f59"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4ca283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de4ca283",
    "outputId": "ecc304a4-7d26-4e4e-c0e1-5e51b99eee14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5b17d",
   "metadata": {
    "id": "10b5b17d"
   },
   "source": [
    "### Q1_a: Print the size of the vocabulary of the above tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48111e36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48111e36",
    "outputId": "e7c72329-3c4d-4fc1-a683-b5e6835ce747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary of the tokenizer is: 30522\n"
     ]
    }
   ],
   "source": [
    "print('The size of the vocabulary of the tokenizer is: {}'.format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901042b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "901042b9",
    "outputId": "2e9b6d81-c553-433d-b95d-ccb41832d098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f022b8ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f022b8ce",
    "outputId": "7628d7b2-16fe-48a2-f1a1-97619022296a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
    "\n",
    "max_input_length = tokenizer.max_model_input_sizes['google-bert/bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6291584a",
   "metadata": {
    "id": "6291584a"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8846cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b8846cd",
    "outputId": "978abad4-2c95-4da8-f1c0-ddea9a2d6370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 66.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1e3fb",
   "metadata": {
    "id": "45a1e3fb"
   },
   "source": [
    "### Q1_b. Print the number of data points in the train, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77adff03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77adff03",
    "outputId": "00dc2970-9f8a-4968-8f54-0409fab2c0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data points in the train sets is: 17500\n",
      "The number of data points in the test sets is: 25000\n",
      "The number of data points in the validation sets is: 7500\n"
     ]
    }
   ],
   "source": [
    "print('The number of data points in the train sets is: {}'.format(len(train_data)))\n",
    "print('The number of data points in the test sets is: {}'.format(len(test_data)))\n",
    "print('The number of data points in the validation sets is: {}'.format(len(valid_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28e4e68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d28e4e68",
    "outputId": "3dc7e196-105e-4ca3-81b5-a621be84b24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.stoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69636c8a",
   "metadata": {
    "id": "69636c8a"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0fe83",
   "metadata": {
    "id": "d5c0fe83"
   },
   "source": [
    "## 2. Model preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864636cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "72d54c1b72f64c56a893ef22d1c2a456",
      "6e252638b6414ecbb49ddb3cced7af3f",
      "99bd52d1b3014ea5b5eb0c94c2b128d9",
      "953c6b3bd30545c4955b3ec9b8efa480",
      "d00446cf1b1347da9ec2ad09664c894a",
      "5a8b3a84b11549a8b610181c46261483",
      "ad645bf761974260a831636049e507d2",
      "0ab17a2660074f84b805e53666238c97",
      "f1289eba8a5448d79e76ca08de17b69c",
      "f3487fc94424479a90f6ac1a0486136c",
      "7bc8b3ffd5e64861bc34ca13d27aea29"
     ]
    },
    "id": "864636cf",
    "outputId": "2ce2d798-7232-4b33-b708-621b57e701bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangjinrui/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3567e413",
   "metadata": {
    "id": "3567e413"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,bert,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        _, hidden = self.rnn(embedded)\n",
    "\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        #hidden = [batch size, hid dim]\n",
    "\n",
    "        output = self.out(hidden)\n",
    "\n",
    "        #output = [batch size, out dim]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8225950",
   "metadata": {
    "id": "c8225950"
   },
   "source": [
    "### Q2a: Instantiate the above model by setting the right hyperparameters.\n",
    "* the BERT embedding (whose weights are frozen)\n",
    "* a bidirectional GRU with 2 layers, with hidden dim 256 and dropout=0.25.\n",
    "* a linear layer on top which does binary sentiment classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce5e740",
   "metadata": {
    "id": "2ce5e740"
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0514cf",
   "metadata": {
    "id": "4f0514cf"
   },
   "source": [
    "### Q2b: Print the number of trainable parameters in this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94229ee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94229ee9",
    "outputId": "9907c9fe-7faf-4c04-a6aa-c7c9acba1880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters in this model is: 112241409\n"
     ]
    }
   ],
   "source": [
    "# insert code here.\n",
    "\n",
    "num_params = sum(i.numel() for i in model.parameters() if i.requires_grad)\n",
    "print('The number of trainable parameters in this model is: {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8352dc0a",
   "metadata": {
    "id": "8352dc0a"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56654222",
   "metadata": {
    "id": "56654222"
   },
   "source": [
    "### Q2c: After freezing the BERT weights/biases, print the number of remaining trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2cac8ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2cac8ad",
    "outputId": "7440d122-44c5-4401-bbfa-a1646a924f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters in this model is: 2759169\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(i.numel() for i in model.parameters() if i.requires_grad)\n",
    "print('The number of trainable parameters in this model is: {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fa04f",
   "metadata": {
    "id": "ee3fa04f"
   },
   "source": [
    "## 3. Train the Model\n",
    "\n",
    "We will use:\n",
    "* the Binary Cross Entropy loss function: `nn.BCEWithLogitsLoss()`\n",
    "* the Adam optimizer\n",
    "\n",
    "and run it for 2 epochs (that should be enough to start getting meaningful results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b1b3ab6",
   "metadata": {
    "id": "4b1b3ab6"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46e1f1d9",
   "metadata": {
    "id": "46e1f1d9"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1487fe04",
   "metadata": {
    "id": "1487fe04"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c08104",
   "metadata": {
    "id": "29c08104"
   },
   "source": [
    "### Q3.\n",
    "* calculating accuracy.\n",
    "* training for a single epoch, and reporting loss/accuracy.\n",
    "* performing an evaluation epoch, and reporting loss/accuracy.\n",
    "* calculating running times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9035769",
   "metadata": {
    "id": "f9035769"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    # Q3a. Compute accuracy (as a number between 0 and 1)\n",
    "\n",
    "    # ...\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "650a5520",
   "metadata": {
    "id": "650a5520"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    # Q3b. Set up the training function\n",
    "\n",
    "    # ...\n",
    "    epoch_loss = 0\n",
    "    epoch_acc =0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        text, label = batch.text.to(device), batch.label.to(device) # Move data to device\n",
    "\n",
    "        optimizer.zero_grad() # zero out any gradient values from the previous iteration\n",
    "\n",
    "        pred = model(text).squeeze(1) # forward propgation\n",
    "\n",
    "        loss = criterion(pred,label) # calculate loss\n",
    "        loss.backward() # back propagation\n",
    "\n",
    "        optimizer.step() # update the weights of our trainable parameters\n",
    "\n",
    "        acc = binary_accuracy(pred,label)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc +=acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a4e7f9c",
   "metadata": {
    "id": "7a4e7f9c"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    # Q3c. Set up the evaluation function.\n",
    "\n",
    "    # ...\n",
    "    epoch_loss = 0\n",
    "    epoch_acc =0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label = batch.text.to(device),batch.label.to(device) # Move data to device\n",
    "\n",
    "            pred = model(text).squeeze(1) # forward propagation\n",
    "\n",
    "            loss = criterion(pred,label) # calculate loss\n",
    "\n",
    "            acc = binary_accuracy(pred,label) #calculate accuracy\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc +=acc.item()\n",
    "\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a170023",
   "metadata": {
    "id": "4a170023"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ea9bac7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ea9bac7",
    "outputId": "aa6315a8-6a13-4c01-a62b-b13721e9ad2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 17m 55s\n",
      "\tTrain Loss: 0.480 | Train Acc: 75.57%\n",
      "\t Val. Loss: 0.301 |  Val. Acc: 87.78%\n",
      "Epoch: 02 | Epoch Time: 18m 4s\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.60%\n",
      "\t Val. Loss: 0.217 |  Val. Acc: 91.28%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Q3d. Perform training/valudation by using the functions you defined earlier.\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "Xd1N7MJijMtS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xd1N7MJijMtS",
    "outputId": "1b393d3c-2835-43c9-f245-71760e96c037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.217 | Test Acc: 91.28%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bii_0kzRnIR0",
   "metadata": {
    "id": "Bii_0kzRnIR0"
   },
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaz_ytjFnGJY",
   "metadata": {
    "id": "aaz_ytjFnGJY"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iF7D6Tidn1BK",
   "metadata": {
    "id": "iF7D6Tidn1BK"
   },
   "source": [
    "### Q4_a Perform sentiment analysis on the following two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "P-XCYUzDnNKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-XCYUzDnNKc",
    "outputId": "ae207e6e-32b0-4d6e-8a94-24f1241ed751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a negative review\n"
     ]
    }
   ],
   "source": [
    "def print_sentiment(sent):\n",
    "    if sent<0.5:\n",
    "        print('It is a negative review')\n",
    "    else:\n",
    "        print('it is a positive review')\n",
    "\n",
    "\n",
    "print_sentiment(predict_sentiment(model, tokenizer, \"Justice League is terrible. I hated it.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "tOSAyuZAnQa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOSAyuZAnQa1",
    "outputId": "4c140715-a1c5-4937-b7b3-0fa29dac28a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a positive review\n"
     ]
    }
   ],
   "source": [
    "print_sentiment(predict_sentiment(model, tokenizer, \"Avengers was great!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rrbIuxzojD_",
   "metadata": {
    "id": "4rrbIuxzojD_"
   },
   "source": [
    "### Q4_b. Perform sentiment analysis on two other movie review fragments of your␣ ↪choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "MIXer4dgohlr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIXer4dgohlr",
    "outputId": "0241c78c-ace6-4b29-b0ac-087e218403a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a positive review\n",
      "It is a negative review\n"
     ]
    }
   ],
   "source": [
    "review_1 = \"I believe that harry potter takes you into a world of magic with aurors and dark wizards alike. It isn't like the films where you know only the bad guys are going to die because good people (sirius black, albus dumbledore etc) and that gives the movie heartfelt emotion that a lot of other movies dont offer. Definetely a top quality film.\"\n",
    "review_2 =\" Way too overrated. Badly written and horrible book, author is antiequality, transphobic and selfmisogynistic who platforms neonazi creators and the movie soundtrack is SO BAD I LITERALLY SWEAR OML. The only thing good abt this series is the hogwarts houses. \"\n",
    "\n",
    "print_sentiment(predict_sentiment(model, tokenizer, review_1))\n",
    "print_sentiment(predict_sentiment(model, tokenizer, review_2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ab17a2660074f84b805e53666238c97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a8b3a84b11549a8b610181c46261483": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e252638b6414ecbb49ddb3cced7af3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a8b3a84b11549a8b610181c46261483",
      "placeholder": "​",
      "style": "IPY_MODEL_ad645bf761974260a831636049e507d2",
      "value": "model.safetensors: 100%"
     }
    },
    "72d54c1b72f64c56a893ef22d1c2a456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e252638b6414ecbb49ddb3cced7af3f",
       "IPY_MODEL_99bd52d1b3014ea5b5eb0c94c2b128d9",
       "IPY_MODEL_953c6b3bd30545c4955b3ec9b8efa480"
      ],
      "layout": "IPY_MODEL_d00446cf1b1347da9ec2ad09664c894a"
     }
    },
    "7bc8b3ffd5e64861bc34ca13d27aea29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "953c6b3bd30545c4955b3ec9b8efa480": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3487fc94424479a90f6ac1a0486136c",
      "placeholder": "​",
      "style": "IPY_MODEL_7bc8b3ffd5e64861bc34ca13d27aea29",
      "value": " 440M/440M [00:06&lt;00:00, 102MB/s]"
     }
    },
    "99bd52d1b3014ea5b5eb0c94c2b128d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ab17a2660074f84b805e53666238c97",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1289eba8a5448d79e76ca08de17b69c",
      "value": 440449768
     }
    },
    "ad645bf761974260a831636049e507d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d00446cf1b1347da9ec2ad09664c894a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1289eba8a5448d79e76ca08de17b69c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3487fc94424479a90f6ac1a0486136c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
